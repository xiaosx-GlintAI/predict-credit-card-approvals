{"cells":[{"source":"![Credit card being held in hand](credit_card.jpg)\n\nCommercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this workbook, you will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.\n\n### The Data\n\nThe data is a small subset of the Credit Card Approval dataset from the UCI Machine Learning Repository showing the credit card applications a bank receives. This dataset has been loaded as a `pandas` DataFrame called `cc_apps`. The last column in the dataset is the target value.","metadata":{},"id":"35aebf2e-0635-4fef-bc9a-877b6a20fb13","cell_type":"markdown"},{"source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) ","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1755285048553,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) ","outputsMetadata":{"0":{"height":50,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"0d5fc006-1f4b-433e-bbf8-779ffe0fe225","nodeType":"const"}},"chartState":{"chartModel":{"modelType":"range","chartId":"id-0vlw6ozwdvy","chartType":"groupedColumn","chartThemeName":"datalabTheme","chartOptions":{"common":{"animation":{"enabled":true}}},"chartPalette":{"fills":["#6568A0","#43D7A4","#4095DB","#FACC5F","#CAE279","#F08083","#5BCDF2","#F099DC","#965858","#7DB64F","#A98954"],"strokes":["#6568A0","#43D7A4","#4095DB","#FACC5F","#CAE279","#F08083","#5BCDF2","#F099DC","#965858","#7DB64F","#A98954"],"up":{"fill":"#459d55","stroke":"#1e652e"},"down":{"fill":"#ef5452","stroke":"#a82529"},"neutral":{"fill":"#b5b5b5","stroke":"#575757"},"altUp":{"fill":"#5090dc","stroke":"#2b5c95"},"altDown":{"fill":"#ffa03a","stroke":"#cc6f10"},"altNeutral":{"fill":"#b5b5b5","stroke":"#575757"}},"cellRange":{"rowStartIndex":null,"rowStartPinned":null,"rowEndIndex":null,"rowEndPinned":null,"columns":[]},"switchCategorySeries":false,"suppressChartRanges":false,"unlinkChart":false,"version":"32.2.2"},"rangeChartModel":{"rangeColumns":[],"switchCategorySeries":false}}}},"lastExecutedByKernel":"51de1b21-ff1b-49f3-86e2-64905c9a840e","visualizeDataframe":false,"version":"ag-charts-v1"},"id":"6e86b1e8-a3fa-4b09-982f-795f218bd1a6","cell_type":"code","execution_count":33,"outputs":[]},{"source":"# Data exploration\n# Print first, last any sample data points\nprint(cc_apps.head())\nprint(cc_apps.tail())\nprint(cc_apps.sample(10))\n\nprint(\"\\n-----Dataframe Info-----\")\ncc_apps.info()\n\nprint(\"\\n-----Columns-----\")\nprint(cc_apps.columns)","metadata":{"executionCancelledAt":null,"executionTime":64,"lastExecutedAt":1755285048617,"lastExecutedByKernel":"51de1b21-ff1b-49f3-86e2-64905c9a840e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Data exploration\n# Print first, last any sample data points\nprint(cc_apps.head())\nprint(cc_apps.tail())\nprint(cc_apps.sample(10))\n\nprint(\"\\n-----Dataframe Info-----\")\ncc_apps.info()\n\nprint(\"\\n-----Columns-----\")\nprint(cc_apps.columns)","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"f42af79b-badf-4763-a111-2dc5131cdedb","outputs":[{"output_type":"stream","name":"stdout","text":"  0      1      2  3  4  5  6     7  8  9   10 11   12 13\n0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  g    0  +\n1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  g  560  +\n2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  g  824  +\n3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  g    3  +\n4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  s    0  +\n    0      1       2  3  4   5   6     7  8  9   10 11   12 13\n685  b  21.08  10.085  y  p   e   h  1.25  f  f   0  g    0  -\n686  a  22.67   0.750  u  g   c   v  2.00  f  t   2  g  394  -\n687  a  25.25  13.500  y  p  ff  ff  2.00  f  t   1  g    1  -\n688  b  17.92   0.205  u  g  aa   v  0.04  f  f   0  g  750  -\n689  b  35.00   3.375  u  g   c   h  8.29  f  f   0  g    0  -\n    0      1       2  3  4   5   6       7  8  9   10 11     12 13\n175  b  27.00   1.500  y  p   w   v   0.375  t  f   0  g   1065  +\n650  b  48.08   3.750  u  g   i  bb   1.000  f  f   0  g      2  -\n280  b  21.17   0.875  y  p   c   h   0.250  f  f   0  g    204  -\n124  a  23.50   9.000  u  g   q   v   8.500  t  t   5  g      0  +\n308  b  27.75   1.290  u  g   k   h   0.250  f  f   0  s      0  -\n24   a  41.17   6.500  u  g   q   v   0.500  t  t   3  g      0  +\n212  b  60.08  14.500  u  g  ff  ff  18.000  t  t  15  g   1000  +\n466  b  31.08   3.085  u  g   c   v   2.500  f  t   2  g     41  -\n132  a  47.42   8.000  u  g   e  bb   6.500  t  t   6  g  51100  +\n453  ?  29.75   0.665  u  g   w   v   0.250  f  f   0  g      0  -\n\n-----Dataframe Info-----\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 690 entries, 0 to 689\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       690 non-null    object \n 1   1       690 non-null    object \n 2   2       690 non-null    float64\n 3   3       690 non-null    object \n 4   4       690 non-null    object \n 5   5       690 non-null    object \n 6   6       690 non-null    object \n 7   7       690 non-null    float64\n 8   8       690 non-null    object \n 9   9       690 non-null    object \n 10  10      690 non-null    int64  \n 11  11      690 non-null    object \n 12  12      690 non-null    int64  \n 13  13      690 non-null    object \ndtypes: float64(2), int64(2), object(10)\nmemory usage: 75.6+ KB\n\n-----Columns-----\nInt64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], dtype='int64')\n"}],"execution_count":34},{"source":"print(\"\\n-----Numerical Columns Summary-----\")\nprint(cc_apps.describe())\n\nprint(\"\\n-----Categorical Columns Summary-----\")\nprint(cc_apps.describe(include=['object']))\n","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1755285048671,"lastExecutedByKernel":"51de1b21-ff1b-49f3-86e2-64905c9a840e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(\"\\n-----Numerical Columns Summary-----\")\nprint(cc_apps.describe())\n\nprint(\"\\n-----Categorical Columns Summary-----\")\nprint(cc_apps.describe(include=['object']))\n","outputsMetadata":{"0":{"height":395,"type":"stream"}}},"cell_type":"code","id":"5a468e43-06db-4e7e-bf3e-5c71b69ef0f3","outputs":[{"output_type":"stream","name":"stdout","text":"\n-----Numerical Columns Summary-----\n               2           7          10             12\ncount  690.000000  690.000000  690.00000     690.000000\nmean     4.758725    2.223406    2.40000    1017.385507\nstd      4.978163    3.346513    4.86294    5210.102598\nmin      0.000000    0.000000    0.00000       0.000000\n25%      1.000000    0.165000    0.00000       0.000000\n50%      2.750000    1.000000    0.00000       5.000000\n75%      7.207500    2.625000    3.00000     395.500000\nmax     28.000000   28.500000   67.00000  100000.000000\n\n-----Categorical Columns Summary-----\n         0    1    3    4    5    6    8    9    11   13\ncount   690  690  690  690  690  690  690  690  690  690\nunique    3  350    4    4   15   10    2    2    3    2\ntop       b    ?    u    g    c    v    t    f    g    -\nfreq    468   12  519  519  137  399  361  395  625  383\n"}],"execution_count":35},{"source":"# Data Preprocessing\n# Replace '?' with NaN\ncc_apps_nanreplaced = cc_apps.replace('?', np.nan)\n\n# Change column type\ncc_apps_nanreplaced[1] = cc_apps_nanreplaced[1].astype(float)\n\nprint(\"\\n-----Change 1 column type from object to float-----\")\nprint(cc_apps_nanreplaced.describe())","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1755285048717,"lastExecutedByKernel":"51de1b21-ff1b-49f3-86e2-64905c9a840e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Replace '?' with NaN\ncc_apps_nanreplaced = cc_apps.replace('?', np.nan)\n\n# Change column type\ncc_apps_nanreplaced[1] = cc_apps_nanreplaced[1].astype(float)\n\nprint(\"\\n-----Change 1 column type from object to float-----\")\nprint(cc_apps_nanreplaced.describe())","outputsMetadata":{"0":{"height":248,"type":"stream"}}},"cell_type":"code","id":"bfd3676c-6234-4de3-aee1-5fa0f5abd85f","outputs":[{"output_type":"stream","name":"stdout","text":"\n-----Change 1 column type from object to float-----\n               1           2           7          10             12\ncount  678.000000  690.000000  690.000000  690.00000     690.000000\nmean    31.568171    4.758725    2.223406    2.40000    1017.385507\nstd     11.957862    4.978163    3.346513    4.86294    5210.102598\nmin     13.750000    0.000000    0.000000    0.00000       0.000000\n25%     22.602500    1.000000    0.165000    0.00000       0.000000\n50%     28.460000    2.750000    1.000000    0.00000       5.000000\n75%     38.230000    7.207500    2.625000    3.00000     395.500000\nmax     80.250000   28.000000   28.500000   67.00000  100000.000000\n"}],"execution_count":36},{"source":"# Create a copy of the NaN replacement DataFrame\ncc_apps_imputed = cc_apps_nanreplaced.copy()\n\n# Data Preprocessing\n# Iterate over each column and impute the most frequent value for object data types and the mean for numeric data types\nfor col in cc_apps_imputed.columns:\n    # Check if the column is of object type\n    if cc_apps_imputed[col].dtypes == \"object\":\n        # Impute with the most frequent value\n        cc_apps_imputed[col] = cc_apps_imputed[col].fillna(\n            cc_apps_imputed[col].value_counts().index[0]\n        )\n    else:\n        cc_apps_imputed[col] = cc_apps_imputed[col].fillna(cc_apps_imputed[col].mean())\n\n# Dummify the categorical features\ncc_apps_encoded = pd.get_dummies(cc_apps_imputed, drop_first=True)\n\n# Extract the last column as your target variable\nX = cc_apps_encoded.iloc[:, :-1].values\ny = cc_apps_encoded.iloc[:, [-1]].values\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Instantiate StandardScaler and use it to rescale X_train and X_test\nscaler = StandardScaler()\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)\n\n# Instantiate a LogisticRegression classifier with default parameter values\nlogreg = LogisticRegression()\n\n# Fit logreg to the train set\nlogreg.fit(rescaledX_train, y_train)\n\n# Use logreg to predict instances from the training set\ny_train_pred = logreg.predict(rescaledX_train)\n\n# Print the confusion matrix of the logreg model\nprint(confusion_matrix(y_train, y_train_pred))\n\n# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001, 0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)\n\n# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Fit grid_model to the data\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n# Summarize results\nbest_train_score, best_train_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_train_score, best_train_params))\n\n# Extract the best model and evaluate it on the test set\nbest_model = grid_model_result.best_estimator_\nbest_score =  best_model.score(rescaledX_test, y_test)\n\nprint(\"Accuracy of logistic regression classifier: \", best_score)","metadata":{"executionCancelledAt":null,"executionTime":3623,"lastExecutedAt":1755285052340,"lastExecutedByKernel":"51de1b21-ff1b-49f3-86e2-64905c9a840e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a copy of the NaN replacement DataFrame\ncc_apps_imputed = cc_apps_nanreplaced.copy()\n\n# Iterate over each column of cc_apps_nans_replaced and impute the most frequent value for object data types and the mean for numeric data types\nfor col in cc_apps_imputed.columns:\n    # Check if the column is of object type\n    if cc_apps_imputed[col].dtypes == \"object\":\n        # Impute with the most frequent value\n        cc_apps_imputed[col] = cc_apps_imputed[col].fillna(\n            cc_apps_imputed[col].value_counts().index[0]\n        )\n    else:\n        cc_apps_imputed[col] = cc_apps_imputed[col].fillna(cc_apps_imputed[col].mean())\n\n# Dummify the categorical features\ncc_apps_encoded = pd.get_dummies(cc_apps_imputed, drop_first=True)\n\n# Extract the last column as your target variable\nX = cc_apps_encoded.iloc[:, :-1].values\ny = cc_apps_encoded.iloc[:, [-1]].values\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Instantiate StandardScaler and use it to rescale X_train and X_test\nscaler = StandardScaler()\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)\n\n# Instantiate a LogisticRegression classifier with default parameter values\nlogreg = LogisticRegression()\n\n# Fit logreg to the train set\nlogreg.fit(rescaledX_train, y_train)\n\n# Use logreg to predict instances from the training set\ny_train_pred = logreg.predict(rescaledX_train)\n\n# Print the confusion matrix of the logreg model\nprint(confusion_matrix(y_train, y_train_pred))\n\n# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001, 0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)\n\n# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Fit grid_model to the data\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n# Summarize results\nbest_train_score, best_train_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_train_score, best_train_params))\n\n# Extract the best model and evaluate it on the test set\nbest_model = grid_model_result.best_estimator_\nbest_score =  best_model.score(rescaledX_test, y_test)\n\nprint(\"Accuracy of logistic regression classifier: \", best_score)","outputsMetadata":{"0":{"height":101,"type":"stream"}}},"cell_type":"code","id":"e258c4c7-e9d5-4a3a-a55e-7e7954f36cf5","outputs":[{"output_type":"stream","name":"stdout","text":"[[185  19]\n [ 32 226]]\nBest: 0.850701 using {'max_iter': 100, 'tol': 0.01}\nAccuracy of logistic regression classifier:  0.8289473684210527\n"}],"execution_count":37}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}